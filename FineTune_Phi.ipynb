{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "avoPthcrfAe2",
        "sG0QjWm9fk6E",
        "jr6EdFdrgM1N",
        "tjb-Tn4ahK8f",
        "gcYpep8jYPGM"
      ],
      "authorship_tag": "ABX9TyPXxqgY3AZMLbqs9Vyks+JF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayank-raj1/Fine-tuned-Phi3Mini/blob/main/FineTune_Phi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tunning Phi-3-mini using DPO\n"
      ],
      "metadata": {
        "id": "CGsFKBKpIdZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "The goal of this notebook is to fine tune Phi-3 mini model using Direct Preference Optimization(DPO) from an synthetic data set, making the model output richer information without explicitly guidance prompt during inference, leding to drastic token savings.\n"
      ],
      "metadata": {
        "id": "hHcnl42pdhfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's Being!"
      ],
      "metadata": {
        "id": "e1gkKvwPejML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Packages\n"
      ],
      "metadata": {
        "id": "avoPthcrfAe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets trl peft bitsandbytes wandb accelerate transformers"
      ],
      "metadata": {
        "id": "A5bmVNBBhnMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "aMMYylFGI99Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftModel,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training\n",
        ")\n",
        "from trl import DPOTrainer, setup_chat_format\n",
        "import bitsandbytes as bnb"
      ],
      "metadata": {
        "id": "mgEDDfbkJUZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the model and tokenizer"
      ],
      "metadata": {
        "id": "sG0QjWm9fk6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "new_model = \"MayankDPOPhi-3--Mini\""
      ],
      "metadata": {
        "id": "ZTo2Ja7rfw8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The bits and bytes config we use for quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")"
      ],
      "metadata": {
        "id": "24ayHUVdgEWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "# Reference model\n",
        "ref_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "ChvhtHEUgHaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']\n",
        ")"
      ],
      "metadata": {
        "id": "K1pwPpXSgJEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading and formating Data Sets\n"
      ],
      "metadata": {
        "id": "jr6EdFdrgM1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "dataset = load_dataset(\"Intel/orca_dpo_pairs\")['train']"
      ],
      "metadata": {
        "id": "2vj7-9oNgQUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.shuffle(seed=42).select(range(1000))"
      ],
      "metadata": {
        "id": "OSRP-zRikr0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -L https://raw.githubusercontent.com/chujiezheng/chat_templates/main/chat_templates/phi-3.jinja"
      ],
      "metadata": {
        "id": "IBM_5mo6ba6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_template = open('phi-3.jinja').read()\n",
        "chat_template = chat_template.replace('    ', '').replace('\\n', '')\n",
        "tokenizer.chat_template = chat_template"
      ],
      "metadata": {
        "id": "0LQA0Miubnzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_format(example):\n",
        "    # Format system\n",
        "    if len(example['system']) > 0:\n",
        "        message = {\"role\": \"system\", \"content\": example['system']}\n",
        "        system = tokenizer.apply_chat_template([message], tokenize=False)\n",
        "    else:\n",
        "        system = \"\"\n",
        "    # Format instruction\n",
        "    message = {\"role\": \"user\", \"content\": example['question']}\n",
        "    prompt = tokenizer.apply_chat_template([message], tokenize=False, add_generation_prompt=True)\n",
        "    # Format chosen answer\n",
        "    chosen = example['chosen'] + \"<|end|>\\n\"\n",
        "    # Format rejected answer\n",
        "    rejected = example['rejected'] + \"<|end|>\\n\"\n",
        "    return {\n",
        "        \"prompt\": system + prompt,\n",
        "        \"chosen\": chosen,\n",
        "        \"rejected\": rejected,\n",
        "    }"
      ],
      "metadata": {
        "id": "vWFgLZ7ib7Lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_columns = dataset.column_names\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "dataset = dataset.map(\n",
        "    dataset_format,\n",
        "    remove_columns=original_columns,\n",
        "    num_proc= os.cpu_count(),\n",
        ")"
      ],
      "metadata": {
        "id": "dZUxwGj_k2FN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# notice the specific llama3 tags like <|eot_id|> which show that the chat template formatting worked\n",
        "dataset[19]"
      ],
      "metadata": {
        "id": "0R5-e3AJk36r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the new fromated data set"
      ],
      "metadata": {
        "id": "ygRfP_uWhAD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the DPO Trainer"
      ],
      "metadata": {
        "id": "tjb-Tn4ahK8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pyPtIsXHhSvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=True,\n",
        "    learning_rate=5e-5,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    max_steps=160,\n",
        "    save_strategy=\"no\",\n",
        "    logging_steps=1,\n",
        "    output_dir=new_model,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    warmup_steps=70,\n",
        "    bf16=True,\n",
        "    report_to=\"wandb\",\n",
        ")"
      ],
      "metadata": {
        "id": "3plbAj20hTlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dpo_trainer = DPOTrainer(\n",
        "    model,\n",
        "    ref_model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    peft_config=peft_config,\n",
        "    beta=0.1,\n",
        "    max_prompt_length=512,\n",
        "    max_length=1024,\n",
        "    force_use_ref_model=True\n",
        ")"
      ],
      "metadata": {
        "id": "XP0Mwewdhdhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "S_NBkPYPhfE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune model with DPO\n",
        "dpo_trainer.train()"
      ],
      "metadata": {
        "id": "3eSt0RYxhgZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving and testing model"
      ],
      "metadata": {
        "id": "gcYpep8jYPGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dpo_trainer.model.save_pretrained(\"final_ckpt\")"
      ],
      "metadata": {
        "id": "N6P3xHQ2YQhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(\"final_ckpt\")"
      ],
      "metadata": {
        "id": "0AkEvdFTYukD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flush memory\n",
        "del dpo_trainer, model, ref_model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "ORBNM1MUYvML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload model in FP16 (instead of NF4)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        ")"
      ],
      "metadata": {
        "id": "XBvqoOOwYy0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
        "tokenizer.chat_template = chat_template"
      ],
      "metadata": {
        "id": "pS2nzq3qZA9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge base model with the adapter\n",
        "model = PeftModel.from_pretrained(base_model, \"final_ckpt\")\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Save model and tokenizer\n",
        "model.save_pretrained(new_model)\n",
        "tokenizer.save_pretrained(new_model)"
      ],
      "metadata": {
        "id": "6j4M50x4Y9Rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MuUYnQqmMJeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pipeline\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=new_model,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "K7lfn9W0ZLn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format prompt\n",
        "message = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant chatbot that provides concise answers.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What are GPUs and why would I use them for machine learning tasks?\"}\n",
        "]\n",
        "tokenizer = AutoTokenizer.from_pretrained(new_model)\n",
        "prompt = tokenizer.apply_chat_template(message, add_generation_prompt=True, tokenize=False)\n",
        "\n",
        "# Generate text\n",
        "sequences = pipeline(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    num_return_sequences=1,\n",
        "    max_length=200,\n",
        ")\n",
        "print(sequences[0]['generated_text'])"
      ],
      "metadata": {
        "id": "3m67_8aNZN4X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}